{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a99597d8",
   "metadata": {},
   "source": [
    "# Importing and Loading COCO, GloVe, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bdf553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2566ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load COCO metadata\n",
    "filename = \"data/captions_train2014.json\"\n",
    "with Path(filename).open() as f:\n",
    "    coco_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b77c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 50.193023920059204s\n"
     ]
    }
   ],
   "source": [
    "# Loading GloVe-200\n",
    "filename = \"data/glove.6B.200d.txt.w2v\"\n",
    "t0 = time.time()\n",
    "glove = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "t1 = time.time()\n",
    "print(\"elapsed %ss\" % (t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed0a5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading ResNet Descriptors\n",
    "with Path('data/resnet18_features.pkl').open('rb') as f:\n",
    "    resnet18_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d93eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(img_url: str) -> Image:\n",
    "    \"\"\"Fetches an image from the web.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_url : string\n",
    "        The url of the image to fetch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PIL.Image\n",
    "        The image.\"\"\"\n",
    "\n",
    "    response = requests.get(img_url)\n",
    "    return Image.open(io.BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56f764ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import numpy as np\n",
    "import mygrad as mg\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "\n",
    "def tokenize(text):\n",
    "    return punc_regex.sub('', text).lower().split()\n",
    "\n",
    "def to_vocab(list_of_counters, k=None, stop_words=tuple()):\n",
    "    \"\"\" \n",
    "    [word, word, ...] -> sorted list of top-k unique words\n",
    "    Excludes words included in `stop_words`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_counters : Iterable[Iterable[str]]\n",
    "    \n",
    "    k : Optional[int]\n",
    "        If specified, only the top-k words are returned\n",
    "    \n",
    "    stop_words : Collection[str]\n",
    "        A collection of words to be ignored when populating the vocabulary\n",
    "    \"\"\"\n",
    "    # <COGINST>\n",
    "    vocab = Counter()\n",
    "    for counter in list_of_counters:\n",
    "        vocab.update(counter)\n",
    "        \n",
    "    for word in set(stop_words):\n",
    "        vocab.pop(word, None)  # if word not in bag, return None\n",
    "    return sorted(i for i,j in vocab.most_common(k))\n",
    "    # </COGINST>\n",
    "\n",
    "def phrase_idf(phrase_vocab, list_of_counters):\n",
    "    N = len(list_of_counters)\n",
    "    #print(N)\n",
    "    #print(phrase_vocab)\n",
    "    \n",
    "    # if term i is not in glove, we set nt[i] = N that way its corresponding idf value is 0\n",
    "    nt = [sum(1 if term in counter else 0 for counter in list_of_counters) for term in phrase_vocab]\n",
    "    nt = np.array(nt, dtype=float)\n",
    "    \n",
    "    nt[nt == 0] = N\n",
    "    \n",
    "    #print(nt)\n",
    "    return np.log10(N / nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2dc354",
   "metadata": {},
   "source": [
    "# Initializing Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "269dd627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database:\n",
    "    def __init__(self):\n",
    "        self.ID_to_descriptor = resnet18_features\n",
    "        self.ID_to_URL = dict()\n",
    "        self.ID_to_captions = defaultdict(list) # key:value | ID --> list of captions corresonding to ID\n",
    "        # NOTE: USE \"glove\" TO CONVERT WORD --> WORD EMBEDDING\n",
    "        \n",
    "        # Getting ID --> URL:\n",
    "        for i in range(len(coco_data[\"images\"])):\n",
    "            ID = coco_data[\"images\"][i]['id']\n",
    "            URL = coco_data[\"images\"][i]['coco_url']\n",
    "            self.ID_to_URL[ID] = URL\n",
    "        \n",
    "        # Getting ID --> Captions\n",
    "        for i in range(len(coco_data[\"annotations\"])):\n",
    "            ID = coco_data[\"annotations\"][i]['image_id']\n",
    "            caption = coco_data[\"annotations\"][i]['caption']\n",
    "            self.ID_to_captions[ID].append(caption)\n",
    "        \n",
    "        # Initialize the dataset\n",
    "        self.make_dataset()\n",
    "        \n",
    "        # Shuffle datasets\n",
    "        self.shuffle_dataset()\n",
    "        \n",
    "        # Making List of caption_counters\n",
    "        self.list_of_caption_counters = []\n",
    "        for ID in self.ID_to_captions:\n",
    "            captions = self.ID_to_captions[ID]\n",
    "            for caption in captions:\n",
    "                self.list_of_caption_counters.append(Counter(tokenize(caption)))\n",
    "    \n",
    "    # This funciton creates the dataset (only call this once during initialization process)\n",
    "    def make_dataset(self):\n",
    "        list_of_IDs = list(self.ID_to_descriptor.keys())\n",
    "        N = len(list_of_IDs)\n",
    "        self.dataset = np.zeros((N, 3), dtype=np.int64) # Shape: N, 3\n",
    "        \n",
    "        for i in range(N):\n",
    "            ID = list_of_IDs[i]\n",
    "            confuser_ID = random.randint(0, N-1)\n",
    "            while ID == confuser_ID: # Just to make sure that the randomly picked confuser ID isn't the same as the img ID; 1/N chance of happening\n",
    "                confuser_ID = random.randint(0, N-1)\n",
    "            caption_index = random.randint(0, len(self.ID_to_captions[ID])-1)\n",
    "            \n",
    "            self.dataset[i][0] = caption_index\n",
    "            self.dataset[i][1] = ID\n",
    "            self.dataset[i][2] = confuser_ID\n",
    "    \n",
    "    '''\n",
    "    This function randomly shuffles the dataset across its rows (each tuplet)\n",
    "    and makes the cuts for the training & validation sets;\n",
    "    call this when you want to shuffle the sets after each epoch.\n",
    "    '''\n",
    "    def shuffle_dataset(self):\n",
    "        np.random.shuffle(self.dataset)\n",
    "        cut = int(self.dataset.shape[0] * (4/5))\n",
    "        self.training_set = self.dataset[0:cut]\n",
    "        self.validation_set = self.dataset[cut:]\n",
    "    \n",
    "    # This function parses the query and returns one word embedding that represents the query\n",
    "    def parse_query(self, phrase):\n",
    "        phrase_vocab = to_vocab([Counter(tokenize(phrase))])\n",
    "\n",
    "        glove_embeddings = [(glove[term] if term in glove else np.zeros(200)) for term in phrase_vocab]\n",
    "        idf = phrase_idf(phrase_vocab, self.list_of_caption_counters)\n",
    "\n",
    "        w_phrase = sum( glove_embeddings[i] * idf[i] for i in range(len(idf)) )\n",
    "\n",
    "        return w_phrase / np.sqrt((w_phrase ** 2).sum(keepdims=True)) # normalized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3844b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73424f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_caption = db.parse_query(\"Hello World it is I.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa732725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_caption.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9ddcea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.98279738e-02,  7.27819130e-02, -4.74345013e-02, -1.03809431e-01,\n",
       "       -3.65419425e-02,  3.88387367e-02, -9.68886986e-02,  5.37098572e-03,\n",
       "        6.08133525e-02,  3.19068842e-02, -4.80326638e-02,  8.43193009e-02,\n",
       "        3.15515548e-02,  8.50647986e-02,  3.07553746e-02,  3.58984130e-03,\n",
       "       -6.91734776e-02,  1.11817487e-01,  4.32791673e-02,  5.25470115e-02,\n",
       "        1.07439242e-01,  4.49819475e-01,  3.19547392e-02, -5.35684824e-02,\n",
       "        2.89007947e-02, -1.57520063e-02,  9.80453752e-03,  9.94066056e-03,\n",
       "       -4.44947556e-02, -3.38789038e-02, -3.37655395e-02,  2.79248320e-02,\n",
       "       -7.52623156e-02, -5.02482206e-02, -8.38084370e-02, -2.70092376e-02,\n",
       "       -1.17717616e-01, -5.26799588e-03, -8.19397997e-03, -1.00290030e-02,\n",
       "       -5.10581918e-02,  5.90292476e-02, -4.90671657e-02,  3.86257172e-02,\n",
       "       -5.01311980e-02,  5.37534244e-02,  1.34071857e-01,  1.09133730e-02,\n",
       "       -1.30879907e-02,  4.76207845e-02,  3.51809859e-02, -2.90490184e-02,\n",
       "        5.57997450e-02,  5.26436530e-02,  2.39472347e-03,  7.93340895e-03,\n",
       "       -3.81848142e-02, -4.03951779e-02, -6.98764548e-02, -3.63961309e-02,\n",
       "        4.54274006e-02, -2.14238316e-02, -5.47737256e-02, -1.20136198e-02,\n",
       "       -2.26845755e-03,  9.84015130e-03, -4.27655056e-02,  1.04341380e-01,\n",
       "        5.52199036e-02,  3.29546370e-02,  1.37563646e-01, -6.47381172e-02,\n",
       "        1.10044880e-02,  2.99527980e-02, -9.60824043e-02, -1.58594351e-03,\n",
       "       -1.02610484e-01,  2.16209497e-02, -1.07879072e-01, -2.98527740e-02,\n",
       "       -2.14935951e-02, -8.52752179e-02,  1.29797589e-02,  6.69959858e-02,\n",
       "       -4.07006107e-02, -1.96631271e-02,  1.97213609e-02, -2.65819672e-02,\n",
       "        3.88321355e-02, -1.88433796e-01, -2.76371045e-03, -5.76935522e-03,\n",
       "        3.65360975e-02, -7.04433545e-02, -4.92652208e-02,  3.69130932e-02,\n",
       "        3.54305841e-02,  2.62768427e-03, -3.88632081e-02, -6.32555783e-02,\n",
       "        6.49883924e-03,  4.44359221e-02, -1.82296168e-02,  4.08302248e-03,\n",
       "        6.20545857e-02, -1.50683960e-02,  6.13766164e-03,  2.01230004e-01,\n",
       "       -1.19006358e-01,  7.22685829e-02, -1.66929550e-02, -6.28296062e-02,\n",
       "        6.22666515e-02, -1.30109526e-02, -4.40244339e-02,  2.27154382e-02,\n",
       "        9.67704598e-03,  9.88988206e-02, -1.31181136e-01, -1.74304489e-02,\n",
       "        1.79771539e-02,  1.47204297e-02,  8.64160061e-02, -5.09278215e-02,\n",
       "        1.27064437e-02, -9.29797590e-02,  5.47921918e-02,  9.71981045e-03,\n",
       "        3.19763198e-02, -8.13218728e-02, -9.73470062e-02, -5.42504787e-02,\n",
       "        4.42469567e-02, -3.97293419e-02, -3.65804918e-02, -1.36099830e-02,\n",
       "       -1.62404012e-02,  2.12481972e-02, -5.18264435e-02,  2.67202687e-03,\n",
       "       -1.18934726e-02, -6.38678297e-02,  1.64585002e-02, -4.93377708e-02,\n",
       "        2.25761071e-01,  7.51727223e-02, -6.47302419e-02, -6.20526783e-02,\n",
       "       -1.90417133e-02,  8.48223194e-02,  8.56513157e-02,  5.77828623e-02,\n",
       "       -7.59287477e-02,  5.09087890e-02,  3.35039459e-02,  3.60747166e-02,\n",
       "       -2.78706085e-02, -1.81494057e-02, -6.37922320e-04,  1.87853426e-02,\n",
       "       -2.21426263e-02,  9.14706010e-03,  1.30506856e-02, -3.37674618e-02,\n",
       "        1.94208790e-02, -1.90142281e-02, -2.41453554e-02,  1.71522703e-02,\n",
       "       -7.25120455e-02, -9.48603451e-03,  3.13700661e-02, -9.96144954e-03,\n",
       "        1.08866125e-01,  7.50231137e-03,  4.38453280e-05,  2.95145363e-02,\n",
       "       -1.26434758e-01, -4.49781641e-02,  1.20251231e-01,  4.99254726e-02,\n",
       "        3.16528708e-01, -4.05022167e-02, -6.44628182e-02,  5.31052351e-02,\n",
       "       -5.78662530e-02, -1.12609833e-01,  2.25052591e-02,  1.11473188e-01,\n",
       "        5.57547174e-02, -2.18300000e-02, -2.22890023e-02, -2.90035736e-02,\n",
       "       -3.77274007e-02,  3.15773040e-02,  1.75463390e-02, -2.92884260e-02,\n",
       "        1.23686856e-02,  2.12853737e-02,  2.89390329e-02, -5.41045610e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716efce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
